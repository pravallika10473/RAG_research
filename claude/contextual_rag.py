import anthropic
import os
from dotenv import load_dotenv
import pickle
import numpy as np
import voyageai
from typing import List, Dict, Any
from tqdm import tqdm
import json
load_dotenv()
client = anthropic.Anthropic(
    api_key=os.getenv("ANTHROPIC_API_KEY"),
)
# Initialise a vector database
class VectorDB:
    def __init__(self, name: str, api_key=None):
        if api_key is None:
            api_key = os.getenv("VOYAGE_API_KEY")
        self.client = voyageai.Client(api_key=api_key)
        self.name = name
        self.embeddings=[]
        self.metadata=[]
        self.query_cache={}
        self.db_path= f"./data/{name}/vector_db.pkl"
    
    def load_data(self,dataset: List[Dict[str,Any]]):
        if self.embeddings and self.metadata:
            print("Vecor database is already loaded. Skipping data loading.")
        if os.path.exists(self.db_path):
            print("Loading vector database from disk.")
            self.load_db()
            return
        
        texts_to_embed=[]
        metadata=[]
        total_chunks= sum(len(doc['chunks']) for doc in dataset)
        with tqdm(total=total_chunks, desc="Processing chunks") as pbar:
            for doc in dataset:
                for chunk in doc['chunks']:
                    texts_to_embed.append(chunk['content'])
                    metadata.append(
                        {
                            'doc_id': doc['doc_id'],
                            'original_uuid': doc['original_uuid'],
                            'chunk_id': chunk['chunk_id'],
                            "original_index": chunk['original_index'],
                            'content': chunk['content']
                        }
                        )
                    pbar.update(1)
        self._embed_and_store(texts_to_embed, metadata)
        self.save_db()

        print(f"Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}")

    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):
        batch_size=128
        with tqdm(total=len(texts), desc="Embedding chunks") as pbar:
            results=[]
            for i in range(0, len(texts), batch_size):
                batch=texts[i:i+batch_size]
                batch_result=self.client.embed(batch,model="voyage-2").embeddings[0]
                results.extend(batch_result)
                pbar.update(len(batch))
        self.embeddings=results
        self.metadata=data
    
    def search(self, query: str, k: int=20) -> List[Dict[str, Any]]:
        if query in self.query_cache:
            query_embedding=self.query_cache[query]
        else:
            query_embedding=self.client.embed([query],model="voyage-2").embeddings[0]
            self.query_cache[query]=query_embedding
        
        if not self.embeddings:
            raise ValueError("No data loaded in the vector database.")
        
        similarities=np.dot(self.embeddings,query_embedding)
        top_indices=np.argsort(similarities)[::-1][:k]

        top_results=[]
        for idx in top_indices:
            result={
                "metadata": self.metadata[idx],
                "similarity": float(similarities[idx])
            }
            top_results.append(result)
        return top_results
    
    def save_db(self):
        data={
            "embeddings": self.embeddings,
            "metadata": self.metadata,
            "query_cache": self.query_cache
        }
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        with open(self.db_path, "wb") as file:
            pickle.dump(data, file)

    def load_db(self):
        if not os.path.exists(self.db_path):
            raise FileNotFoundError(f"Vector database file not found at {self.db_path}")
        
        with open(self.db_path, "rb") as file:
            data=pickle.load(file)
        self.embeddings=data["embeddings"]
        self.metadata=data["metadata"]
        self.query_cache=data["query_cache"]
    
    def validate_embedded_chunks(self):
        unique_contents = set()
        for meta in self.metadata:
            unique_contents.add(meta['content'])
    
        print(f"Validation results:")
        print(f"Total embedded chunks: {len(self.metadata)}")
        print(f"Unique embedded contents: {len(unique_contents)}")
    
        if len(self.metadata) != len(unique_contents):
            print("Warning: There may be duplicate chunks in the embedded data.")
        else:
            print("All embedded chunks are unique.")


# Load your transformed dataset
with open('codebase_chunks.json', 'r') as f:
    transformed_dataset = json.load(f)

# Initialize the VectorDB
base_db = VectorDB("base_db")

# Load and process the data
base_db.load_data(transformed_dataset)
